{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd03f51cb7c77d18eeae37d8743e6a8965b321790527a9d46ec999dcb7b2a990bf5",
   "display_name": "Python 3.6.9 64-bit ('.venv')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Research\n",
    "\n",
    "The initial goal is to determine the different variables that we can change to see how effieiency changes.\n",
    "As of now, these are:\n",
    "- GPU Frequency\n",
    "- CPU Frequency\n",
    "- Memory Frequency\n",
    "- Matrix Size\n",
    "- Deep Learning Accelerators (DLAs)\n",
    "- Tensor Cores\n",
    "- Data Types (Half, Float, Double)\n",
    "\n",
    "Ideally the goal would be to test all combinations of them, but as there are over 30,000 combinations it's unreasonable.\n",
    "\n",
    "## AGX Info\n",
    "\n",
    "```\n",
    "$ cat /etc/nv_tegra_release \n",
    "# R32 (release), REVISION: 4.4, GCID: 23942405, BOARD: t186ref, EABI: aarch64, DATE: Fri Oct 16 19:37:08 UTC 2020\n",
    "```\n",
    "\n",
    "```\n",
    "$ nvcc -V\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2019 NVIDIA Corporation\n",
    "Built on Wed_Oct_23_21:14:42_PDT_2019\n",
    "Cuda compilation tools, release 10.2, V10.2.89\n",
    "```\n",
    "\n",
    "## Nano Info\n",
    "Todo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In an attempt to simplify things I created a templated function, `benchmark`, that can take `__half`, `float`, or `double` types.\n",
    "\n",
    "```C++\n",
    "template <typename T>\n",
    "void benchmark(int min_dim, int max_dim) {\n",
    "    cublasHandle_t handle;\n",
    "    cublasCreate(&handle);\n",
    "\n",
    "    T* h_A = (T*)malloc(max_dim * max_dim * sizeof(T));\n",
    "    T* h_B = (T*)malloc(max_dim * max_dim * sizeof(T));\n",
    "    T* h_C = (T*)malloc(max_dim * max_dim * sizeof(T));\n",
    "\n",
    "    fill_random(h_A, max_dim);\n",
    "    fill_random(h_B, max_dim);\n",
    "\n",
    "    T *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, max_dim * max_dim * sizeof(T));\n",
    "    cudaMalloc(&d_B, max_dim * max_dim * sizeof(T));\n",
    "    cudaMalloc(&d_C, max_dim * max_dim * sizeof(T));\n",
    "\n",
    "    cudaMemcpy(d_A, h_A, max_dim * max_dim * sizeof(T), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, max_dim * max_dim * sizeof(T), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_C, h_C, max_dim * max_dim * sizeof(T), cudaMemcpyHostToDevice);\n",
    "\n",
    "    for (int dim = min_dim; dim <= max_dim; dim *= 2) {\n",
    "        gemm(handle, dim, d_A, d_B, d_C);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The `gemm` function is templated to run the correct CUDA GEMM function for the data types.\n",
    "```C++\n",
    "...\n",
    "template <>\n",
    "void gemm(cublasHandle_t handle, int dim, __half *d_A, __half *d_B, __half *d_C) {\n",
    "    printf(\"Half - %d\\n\", dim);\n",
    "    __half alpha = __float2half(1.0f);\n",
    "    __half beta = __float2half(1.0f);\n",
    "    cublasHgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, dim, dim, dim, &alpha, d_A, dim, d_B, dim, &beta, d_C, dim);\n",
    "}\n",
    "\n",
    "template <>\n",
    "void gemm(cublasHandle_t handle, int dim, float *d_A, float *d_B, float *d_C) {\n",
    "    printf(\"Float - %d\\n\", dim);\n",
    "    float alpha = 1;\n",
    "    float beta = 1;\n",
    "    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, dim, dim, dim, &alpha, d_A, dim, d_B, dim, &beta, d_C, dim);\n",
    "}\n",
    "...\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}